---
title: "Coursera - Data Science Track"
author: "Ken"
date: "March 4, 2016"
output: html_document
---

# Practical Machine Learning 

## Prediction Assignment 

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. 
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 
The model will predict the "classe" of the excercise. 

More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 
  
Install the required packages.  

```{r,cache=FALSE, warning = FALSE, message = FALSE, echo=FALSE}
set.seed(968)
library(dplyr)
library(ggplot2)
library(caret)
library(caretEnsemble)
library(randomForest)
library(ada)
library(doMC)
registerDoMC(cores = 3)
```
  
### Data Reading

Any data manipulation done on the training data should also be done on the testing data to ensure consistency, so the model is able to make predictions. 
Read in both the training and testing data.

```{r,cache=TRUE}
# set working directory 
setwd("~/Documents/Machine Learning/practical ml/assignment")

# Read in the data from the .csv files, avoid converting all Strings to factors.
# Also set the NA strings for some strange looking items found in the data.
trainInput <- read.csv("pml-training.csv", na.strings=c("NA", "#DIV/0!"), stringsAsFactors=FALSE)
testInput <- read.csv("pml-testing.csv", na.strings=c("NA", "#DIV/0!"), stringsAsFactors=FALSE)
```

### Clean and prepare

Merge test and train sets. 
For this to work the sets need the same columns, so that gets fixed first. 
Next use only the useful columns as described in the course. 
And use rows that are complete (no missing data). 

The result is only numeric values, so no need to fix that. 
Character vectors would be less ideal for a machine learning algorithm. 

```{r,cache=TRUE}
# Add an empty column for "classe" to the test set
testInput$classe <- NA

# Add an empry column for "problem_id" to the train set
trainInput$problem_id <- NA

# Combine the datasets into one large one for consistent data manipulation
dataset <- rbind(trainInput,testInput)

# Select only the columns with arm, belt or dumbell in their name
dataset <- select(dataset, matches("arm|belt|dumbell|classe|problem_id"))

# Use only columns without NA values
featureCols <- c(colnames(dataset[colSums(is.na(dataset)) == 0]), colnames(dataset[115:116]))
myFeatures <- dataset[featureCols]

# Filter out the records with missing values
myFeatures %>% filter(complete.cases(.))
```

### Spliting data

Split the dataset in a training and validation set.

```{r, cache=TRUE}
# Partition into training and validation sets:
inTrain <- createDataPartition(y = myFeatures$classe, p = 0.6, list = FALSE)

training <- myFeatures[inTrain,]
validation <- myFeatures[-inTrain,]

```

### Fit

Started with one model (random forests). 
Due to limited time, no extra models where developed and trained. 
I would have liked to add a Ada boost, and create an ensemble of RF and Ada. 
But results of the RF are good, and time limited. 

```{r,cache=TRUE,message=FALSE}
# Model: Random Forests
# using cross validation resampling. 

modelRF <- train(as.factor(classe)~., data = training[1:40],  
                  method = "rf", na.action = na.omit, 
                  trControl = trainControl(method = "cv"),
                  allowParallel = TRUE)

print(modelRF)
```

Seems like a good model, with an accuracy of 98%.

### Predict 

Run the trained model on the test data. 

```{r,cache=TRUE,message=FALSE}
# Get the test set back
testing <- myFeatures %>% filter(is.na(myFeatures$problem_id) == FALSE)

# and predict 
predictionsRF <- predict(modelRF, testing)
print(predictionsRF)

#extractPrediction(predictionsRF, testing)

```

### Evaluate

Using confusion matrices for evaluation
  
```{r,cache=TRUE}
# Model 1
validateRF <- predict(modelRF, validation)
confusionMatrix(validateRF, validation$classe)
```

### Variable Importance Plot

```{r,cache=TRUE,fig.cap="Variable importance plot, illustrating the importance of each variable in partitioning the data into the defined classes. Variables to the upper right-side are more important when splitting the data, while the bottom left-side of the plot contains less important variables that are still in the final model."}
varImpPlot(modelRF$finalModel,main="Variable Importance Plot",pch=19)
title(xlab="Variable Importance for Data Partitioning\n")
```
